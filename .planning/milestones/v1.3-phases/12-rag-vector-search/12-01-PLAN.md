---
phase: 12-rag-vector-search
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .agent/skills/rag-vector-search/SKILL.md
autonomous: true
requirements:
  - RAG-01
  - RAG-02
  - RAG-03
  - RAG-04
  - RAG-05

must_haves:
  truths:
    - "Developer can follow the Quick Start and run embed-and-retrieve in under 15 minutes — console output of retrieved chunks is visible"
    - "The Quick Start schema includes HNSW index and embedding_model_version column — no retrofit needed after initial setup"
    - "Similarity threshold filter (score >= 0.75) appears in the Quick Start retrieve step — not as a footnote"
    - "A chunking strategy decision table with token budget math is readable in Section 2"
    - "The RRF hybrid search SQL CTE is in the SKILL.md main body (not only in references/)"
    - "At least 4 named Gotchas with warning signs and fixes are present (Missing HNSW Index, Embedding Model Drift, Chunking Pitfalls, HNSW Fragmentation)"
    - "The Evaluation section explains recall@k and MRR, links to eval.ts, and states thresholds (recall@5 > 0.8 = production-ready)"
  artifacts:
    - path: ".agent/skills/rag-vector-search/SKILL.md"
      provides: "Complete RAG skill at auth-systems depth (~437 lines)"
      min_lines: 380
      contains: "Quick Start"
    - path: ".agent/skills/rag-vector-search/SKILL.md"
      provides: "HNSW schema in Quick Start"
      contains: "USING hnsw"
    - path: ".agent/skills/rag-vector-search/SKILL.md"
      provides: "RRF CTE inline"
      contains: "rrf_score"
    - path: ".agent/skills/rag-vector-search/SKILL.md"
      provides: "Gotchas section"
      contains: "Gotcha"
  key_links:
    - from: ".agent/skills/rag-vector-search/SKILL.md"
      to: ".agent/skills/rag-vector-search/eval.ts"
      via: "markdown link"
      pattern: "eval\\.ts"
    - from: ".agent/skills/rag-vector-search/SKILL.md"
      to: ".agent/skills/rag-vector-search/references/retrieval-patterns.md"
      via: "See reference link"
      pattern: "references/retrieval-patterns"
---

<objective>
Rewrite SKILL.md from 92 lines to auth-systems depth (~437 lines).

Purpose: The existing skill is a thin shell pointing to references/. Phase 12 promotes the production-essential patterns into the main body: a copy-pasteable Quick Start with production-safe schema (HNSW index + embedding_model_version column), a chunking strategy decision table with token budget math, RRF hybrid search SQL inline, an evaluation section linked to eval.ts, and 4 named Gotchas. Developer can reach working embed-and-retrieve in under 15 minutes.

Output: .agent/skills/rag-vector-search/SKILL.md — complete, ~437-line skill at auth-systems structural depth.
</objective>

<execution_context>
@/home/ollie/.claude/get-shit-done/workflows/execute-plan.md
@/home/ollie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/12-rag-vector-search/12-CONTEXT.md
@.planning/phases/12-rag-vector-search/12-RESEARCH.md
@.agent/skills/rag-vector-search/SKILL.md
@.agent/skills/auth-systems/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite SKILL.md — Quick Start + Sections 1-2 (Schema + Chunking)</name>
  <files>.agent/skills/rag-vector-search/SKILL.md</files>
  <action>
Rewrite SKILL.md completely. Begin with the existing frontmatter (name, description) — update description to reflect the new scope (pgvector-native, hybrid search, eval patterns). Structure:

**Frontmatter (keep name: rag-vector-search, update description):**
description: "Use when implementing retrieval-augmented generation or semantic search with PostgreSQL. Covers production-safe pgvector schema (HNSW index, model version column), embedding with OpenAI text-embedding-3-small, chunking strategies with token budget math, hybrid search with RRF fusion, retrieval evaluation (recall@k, MRR), and 4 named production Gotchas."

**Quick Start section** (target: developer running in < 15 minutes, no web server needed):

Opening callout: "Assumes Postgres with pgvector extension available (`CREATE EXTENSION vector` is the starting point). Install: `npm install openai pgvector`."

Show the full production schema SQL as commented block inside a TypeScript file — not a separate SQL file. The TypeScript script is the Quick Start artifact. Schema must include:

- `document_chunks` table with: id UUID, source_id TEXT NOT NULL, content TEXT NOT NULL, embedding vector(1536), embedding_model_version TEXT NOT NULL, search_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED, created_at TIMESTAMPTZ DEFAULT NOW()
- HNSW index: `CREATE INDEX ON document_chunks USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);`
- GIN index: `CREATE INDEX ON document_chunks USING gin(search_vector);`
- Model version index: `CREATE INDEX ON document_chunks (embedding_model_version);`

Then embed-and-store code (TypeScript, complete with imports):

- Import: `import OpenAI from 'openai'; import pgvector from 'pgvector/prisma'; import { PrismaClient } from '@prisma/client';`
- Use `pgvector.toSql(item.embedding)` — NOT `JSON.stringify(embedding)`. Add inline comment: `// pgvector-node — use toSql(), not JSON.stringify() — different wire format`
- Model hardcoded to `'text-embedding-3-small'`
- embedding_model_version hardcoded to `'text-embedding-3-small-v1'`
- Use `db.$transaction([...])` with `db.$executeRaw` template literal

Then retrieve code (complete with imports, inline in the same script):

- Query: cosine distance `1 - (embedding <=> ...)` for score
- `WHERE embedding_model_version = 'text-embedding-3-small-v1'`
- `LIMIT 5`
- Filter: `results.filter((r) => r.score >= 0.75)` — add inline comment: `// 0.75 threshold: prevents garbage context from reaching LLM`
- `console.log` each result with score and content preview
- `await db.$disconnect()` at end

Quick Start ends with: "Run: `npx tsx embed-and-retrieve.ts`" and note: "You should see 1–3 chunks printed with scores ≥ 0.75."

Add note: `> Self-hosting path uses Prisma $executeRaw. See references/embedding-pipelines.md for batch embedding with retry.`

---

**Section 1: Schema**

Expand on Quick Start schema with explanations:

- Why `embedding_model_version` is NOT OPTIONAL: without it, model drift causes silent data corruption (preview Gotcha 2)
- HNSW vs IVFFlat decision: table with 3 rows (new schema preference, >1M rows, build time budget)
- HNSW parameters: m=16, ef_construction=64 (balanced default), ef_search note: "Default ef_search=40. If LIMIT exceeds 40, run `SET hnsw.ef_search = N` where N ≥ LIMIT before the query."
- Generated tsvector column: explain why `GENERATED ALWAYS AS ... STORED` avoids manual sync bugs
- Include the `CREATE INDEX CONCURRENTLY` variant for live tables

**Section 2: Chunking**

Decision table (4 rows):

| Strategy   | When to Use                                | Chunk Size | Overlap                | Tooling                                  |
| ---------- | ------------------------------------------ | ---------- | ---------------------- | ---------------------------------------- |
| Fixed-size | Default; uniform content (articles, docs)  | 512 tokens | 50–100 tokens (10–20%) | `tiktoken` or word-count approx          |
| Semantic   | Structured content (markdown, legal, code) | Variable   | ~10–15%                | Python: `RecursiveCharacterTextSplitter` |

Token budget math (REQUIRED — locked decision):

```
chunk_tokens × topK + system_prompt_tokens < model_context_limit
Example: 512 × 10 + 1,000 = 6,120 tokens — well within gpt-4o's 128K context
```

Note: "512 characters ≠ 512 tokens. Use tiktoken for exact counts, or approximate at 0.75 tokens/character."

Overlap rule: "stride = 10–20% of chunk size (e.g. 512-token chunk → 50–100 token overlap). Smaller overlap risks splitting key sentences; larger overlap increases embedding cost."

Python aside (practical note, not primary path):

```python
# Python ingestion pipeline — semantic boundary awareness
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,    # characters (use tiktoken for token-precise control)
    chunk_overlap=64,  # ~12.5% overlap
    length_function=len,
)
chunks = splitter.split_text(document_text)
```

Note: "Semantic chunking improves recall ~9% for structured documents (Weaviate, 2025). For plain prose, fixed-size is sufficient."

Reference link: `> See references/embedding-pipelines.md for the full TypeScript chunkText() function and batch embed with retry.`
</action>
<verify>
<automated>wc -l .agent/skills/rag-vector-search/SKILL.md</automated>
<manual>Confirm Quick Start section exists, HNSW index SQL is present, pgvector.toSql() is used (not JSON.stringify), score >= 0.75 filter is in retrieve step, decision table for chunking is present with token budget math formula</manual>
</verify>
<done>SKILL.md has Quick Start with full production schema (HNSW + model version column), embed-and-store using pgvector.toSql(), retrieve with 0.75 threshold and console output, Section 1 Schema with HNSW vs IVFFlat table, Section 2 Chunking with decision table and token budget formula</done>
</task>

<task type="auto">
  <name>Task 2: Complete SKILL.md — Sections 3-4 + Gotchas</name>
  <files>.agent/skills/rag-vector-search/SKILL.md</files>
  <action>
Append Sections 3 and 4 and the Gotchas section to the SKILL.md written in Task 1. Do NOT rewrite what Task 1 created — append only.

**Section 3: Hybrid Search (RRF Fusion)**

Opening: "Hybrid search combines vector similarity and full-text search. Use RRF (Reciprocal Rank Fusion) — not weighted score addition. Weighted addition (`0.7 × cosine + 0.3 × ts_rank`) conflates incompatible score scales. RRF works on ranks so no normalization is needed."

Show the full RRF SQL CTE inline in the main SKILL.md body (this is a locked decision — must be here, not only in references/):

```sql
-- Hybrid search: vector similarity + full-text, merged via RRF (k=60 from Cormack et al.)
WITH vector_search AS (
  SELECT id, ROW_NUMBER() OVER (ORDER BY embedding <=> $1::vector) AS rank
  FROM document_chunks
  WHERE embedding_model_version = 'text-embedding-3-small-v1'
  LIMIT 20
),
text_search AS (
  SELECT id, ROW_NUMBER() OVER (ORDER BY ts_rank(search_vector, plainto_tsquery('english', $2)) DESC) AS rank
  FROM document_chunks
  WHERE search_vector @@ plainto_tsquery('english', $2)
  LIMIT 20
)
SELECT
  COALESCE(v.id, t.id) AS id,
  1.0 / (60 + COALESCE(v.rank, 1000)) + 1.0 / (60 + COALESCE(t.rank, 1000)) AS rrf_score
FROM vector_search v
FULL OUTER JOIN text_search t ON v.id = t.id
ORDER BY rrf_score DESC
LIMIT 10;
```

Add a TypeScript wrapper function `hybridSearch(query: string, topK = 10)` that passes the query embedding as `$1` (via `pgvector.toSql()`) and the query text as `$2` to this SQL using `db.$queryRaw`.

Note: "k=60 is the standard RRF constant from Cormack et al. (2009). It generalizes well across datasets and is the community default."

Reference link: `> See references/retrieval-patterns.md for RAG prompt assembly and two-stage re-ranking patterns.`

**Section 4: Evaluation**

Opening: "Before going to production, verify your pipeline meets minimum recall thresholds."

Explain the two metrics:

- **recall@k**: fraction of expected chunks appearing in top-k results. Formula: `hits / expected.length` where hits = expected chunks found in top-k. Threshold: recall@5 > 0.8 = production-ready; below 0.6 = investigate chunking or embedding.
- **MRR (Mean Reciprocal Rank)**: inverse rank of the first relevant result. Formula: `1 / rank_of_first_relevant`. Threshold: MRR > 0.7 = relevant result in top 2 on average.

Show the eval.ts path and how to run it:

```
Run: npx tsx .agent/skills/rag-vector-search/eval.ts
```

Brief explanation: "eval.ts contains a golden set of 3–5 hardcoded queries with expected chunk IDs. No DB seeding required — uses data from the Quick Start. Outputs recall@5 and MRR for each query."

Reference link: `> See .agent/skills/rag-vector-search/eval.ts for the full runnable evaluation script.`

---

**Gotchas section** (heading: `## Gotchas / Pitfalls`)

Follow auth-systems format: numbered gotchas, each with a short description, Warning signs list, and Fix code block.

### Gotcha 1: Missing HNSW Index (Sequential Scan Regression)

Without an HNSW index, every similarity query does a full sequential scan. Tolerable on 10K rows; degrades to >1s on 100K+ rows.

**Why it happens:** Developers prototype with small datasets, ship without the index, miss it until load increases.

**Warning signs:**

- `EXPLAIN ANALYZE` shows `Seq Scan` instead of `Index Scan using hnsw_...`
- Query latency grows linearly with row count

**Fix:**

```sql
-- Check existing indexes
SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'document_chunks';
-- Add HNSW index on live table (CONCURRENTLY avoids write lock)
CREATE INDEX CONCURRENTLY ON document_chunks
  USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);
```

### Gotcha 2: Embedding Model Drift (Silent Data Corruption)

Switching embedding models (e.g. `text-embedding-3-small` to `text-embedding-3-large`) mixes incompatible vectors in the same table. Similarity searches return nonsense — vectors from different models occupy completely different geometric spaces.

**Why it happens:** The embedding column alone has no metadata about which model produced it. Mixed vectors are indistinguishable at query time.

**Warning signs:**

- Similarity scores cluster around 0.5 (mixed-model vectors produce ~random similarities)
- recall@5 drops suddenly without code changes
- Two developers embed the same document and get different `embedding` values

**Fix:**

```sql
-- Identify stale rows
SELECT embedding_model_version, COUNT(*) FROM document_chunks GROUP BY 1;
-- After re-embedding batch, update version column
UPDATE document_chunks
SET embedding = $1, embedding_model_version = 'text-embedding-3-large-v1'
WHERE id = $2;
```

### Gotcha 3: Chunking Pitfalls (Context Loss and Budget Overrun)

Two common failure modes:

1. **No overlap**: key sentence at chunk boundary is split — neither chunk contains full context. LLM hallucinates from incomplete context.
2. **Chunks too large**: `topK=10` with `chunk_size=1024` tokens = 10,240 tokens of context. With a 16K or 32K context model, or a large system prompt, this silently truncates the prompt.

**Why it happens:** Default splitters use character counts, not token counts. 512 characters ≈ 384 tokens (not 512).

**Warning signs:**

- LLM answers reference half-sentences or cut-off facts
- Token usage per LLM call spikes unexpectedly
- `tiktoken` reveals chunks are 2–3× larger than expected

**Fix:** Use `tiktoken` (Python/npm) for token-precise chunking. Enforce overlap rule: stride = 10–20% of chunk size. Verify budget: `chunk_tokens × topK + system_prompt_tokens < model_context_limit` before embedding.

### Gotcha 4: HNSW Index Fragmentation (Recall Degrades Over Time)

HNSW is a graph structure. Deleted or updated rows leave "dead nodes" in the graph. Over time HNSW traverses abandoned paths during search, silently degrading recall. No error is raised.

**Why it happens:** HNSW indices are not self-healing. Dead tuples accumulate between VACUUM runs.

**Warning signs:**

- recall@5 drifts downward over weeks without schema or code changes
- `pgstattuple` shows high dead-tuple count on `document_chunks`
- After a bulk-delete + re-embed operation, search quality noticeably drops

**Fix:** Run `VACUUM ANALYZE document_chunks` regularly (pg auto-vacuum covers most workloads). After bulk re-embed: `REINDEX INDEX CONCURRENTLY idx_document_chunks_embedding;`

---

**Version Context** (keep a short table at the end):

| Library                         | Version | Notes                                                                                                        |
| ------------------------------- | ------- | ------------------------------------------------------------------------------------------------------------ |
| `pgvector` (Postgres extension) | 0.8.x   | HNSW is the preferred default index (no training step, better recall); IVFFlat valid for >1M rows            |
| `openai` (npm)                  | 4.x     | `text-embedding-3-small` (1536 dims, cost-optimal default); `text-embedding-3-large` (3072 dims, ~6× cost)   |
| `pgvector` (npm, pgvector-node) | latest  | `pgvector/prisma` import — provides `toSql()` for Prisma `$executeRaw`                                       |
| Prisma                          | 5.x     | pgvector requires raw SQL (`$executeRaw` / `$queryRaw`) — Prisma does not natively support the `vector` type |

  </action>
  <verify>
    <automated>grep -c "rrf_score\|HNSW\|recall\|Gotcha\|embedding_model_version" .agent/skills/rag-vector-search/SKILL.md</automated>
    <manual>Confirm: RRF CTE SQL is in main body, 4 Gotchas present with Warning signs and Fix blocks, eval.ts run command is shown, MRR and recall@k thresholds are stated, Version Context table at end</manual>
  </verify>
  <done>SKILL.md contains: RRF hybrid search SQL CTE inline in Section 3, TypeScript hybridSearch() wrapper, Section 4 evaluation with recall@k and MRR thresholds linked to eval.ts, 4 named Gotchas (Missing HNSW Index, Embedding Model Drift, Chunking Pitfalls, HNSW Fragmentation) each with Warning signs and Fix. Final line count ≥ 380.</done>
</task>

</tasks>

<verification>
After both tasks complete:

1. `wc -l .agent/skills/rag-vector-search/SKILL.md` — must be ≥ 380 lines
2. `grep "USING hnsw" .agent/skills/rag-vector-search/SKILL.md` — must match (HNSW in Quick Start)
3. `grep "pgvector.toSql" .agent/skills/rag-vector-search/SKILL.md` — must match (not JSON.stringify)
4. `grep "score >= 0.75" .agent/skills/rag-vector-search/SKILL.md` — must match (threshold in Quick Start)
5. `grep "rrf_score" .agent/skills/rag-vector-search/SKILL.md` — must match (RRF SQL inline)
6. `grep -c "### Gotcha" .agent/skills/rag-vector-search/SKILL.md` — must be 4
7. `grep "recall@5 > 0.8" .agent/skills/rag-vector-search/SKILL.md` — must match (evaluation threshold)
8. `grep "chunk_tokens" .agent/skills/rag-vector-search/SKILL.md` — must match (token budget math)
   </verification>

<success_criteria>

- SKILL.md ≥ 380 lines
- Quick Start: full schema SQL (HNSW + model version + GIN), embed with pgvector.toSql(), retrieve with score >= 0.75 filter, console output
- Section 1 Schema: HNSW vs IVFFlat table, ef_search note, CONCURRENTLY variant
- Section 2 Chunking: fixed-size vs semantic decision table, token budget formula, Python RecursiveCharacterTextSplitter aside
- Section 3 Hybrid Search: RRF CTE SQL in main body, TypeScript wrapper
- Section 4 Evaluation: recall@k and MRR metrics, thresholds, eval.ts run command
- 4 named Gotchas with Warning signs and Fix blocks
- Version Context table
- All 5 phase requirements (RAG-01 through RAG-05) addressed
  </success_criteria>

<output>
After completion, create `.planning/phases/12-rag-vector-search/12-01-SUMMARY.md`
</output>
