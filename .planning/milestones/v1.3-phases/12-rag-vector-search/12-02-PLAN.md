---
phase: 12-rag-vector-search
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - .agent/skills/rag-vector-search/eval.ts
  - .agent/skills/rag-vector-search/references/embedding-pipelines.md
  - .agent/skills/rag-vector-search/references/retrieval-patterns.md
autonomous: true
requirements:
  - RAG-03
  - RAG-05

must_haves:
  truths:
    - "eval.ts is a runnable script — `npx tsx eval.ts` executes without crashing and prints recall@5 and MRR for each golden query"
    - "eval.ts golden set has 3–5 hardcoded queries with expected chunk IDs (no DB seeding required)"
    - "references/embedding-pipelines.md schema uses HNSW index (not IVFFlat as the default) and includes the document_chunks table name with embedding_model_version column"
    - "references/retrieval-patterns.md includes RRF CTE as the primary hybrid search pattern (weighted addition demoted to 'simpler alternative' callout)"
  artifacts:
    - path: ".agent/skills/rag-vector-search/eval.ts"
      provides: "Runnable RAG evaluation script with recall@k and MRR"
      min_lines: 60
      contains: "recallAtK"
    - path: ".agent/skills/rag-vector-search/eval.ts"
      provides: "Golden set with 3-5 queries"
      contains: "GOLDEN_SET"
    - path: ".agent/skills/rag-vector-search/references/embedding-pipelines.md"
      provides: "Updated schema with HNSW index"
      contains: "USING hnsw"
    - path: ".agent/skills/rag-vector-search/references/retrieval-patterns.md"
      provides: "RRF CTE as primary hybrid search pattern"
      contains: "rrf_score"
  key_links:
    - from: ".agent/skills/rag-vector-search/eval.ts"
      to: "pgvector/prisma"
      via: "import pgvector"
      pattern: "pgvector"
    - from: ".agent/skills/rag-vector-search/references/retrieval-patterns.md"
      to: "document_chunks"
      via: "SQL table reference"
      pattern: "document_chunks"
---

<objective>
Create eval.ts and update both references/ files to align with Phase 12's production patterns.

Purpose: eval.ts gives developers an objective quality signal (recall@5 > 0.8 = production-ready) within minutes of setting up their pipeline. The references/ files are the implementation depth layer — embedding-pipelines.md needs its schema updated from IVFFlat to HNSW (the Quick Start table name is also `document_chunks` not `embeddings`), and retrieval-patterns.md needs the RRF CTE added as the recommended approach.

Output: eval.ts (runnable evaluation script), updated references/embedding-pipelines.md, updated references/retrieval-patterns.md.
</objective>

<execution_context>
@/home/ollie/.claude/get-shit-done/workflows/execute-plan.md
@/home/ollie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/12-rag-vector-search/12-CONTEXT.md
@.planning/phases/12-rag-vector-search/12-RESEARCH.md
@.agent/skills/rag-vector-search/references/embedding-pipelines.md
@.agent/skills/rag-vector-search/references/retrieval-patterns.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create eval.ts — runnable RAG evaluation script</name>
  <files>.agent/skills/rag-vector-search/eval.ts</files>
  <action>
Create `.agent/skills/rag-vector-search/eval.ts` as a NEW file (does not exist yet). This is a standalone runnable script — `npx tsx .agent/skills/rag-vector-search/eval.ts`.

File structure and requirements:

**File header comment:**

```typescript
// .agent/skills/rag-vector-search/eval.ts
// Run: npx tsx .agent/skills/rag-vector-search/eval.ts
//
// RAG evaluation script. Requires the Quick Start data to be inserted first
// (see SKILL.md Quick Start section — 3 sample documents are embedded there).
//
// Metrics:
//   recall@k  — fraction of expected chunks in top-k results (threshold: >0.8 for production)
//   MRR       — mean reciprocal rank of first relevant result (threshold: >0.7)
//
// No DB seeding required — uses hardcoded chunk IDs from Quick Start inserts.
// To get the chunk IDs: SELECT id, content FROM document_chunks LIMIT 10;
```

**Imports:**

```typescript
import OpenAI from "openai";
import pgvector from "pgvector/prisma";
import { PrismaClient } from "@prisma/client";
```

**GOLDEN_SET constant** — 3 queries matching the Quick Start documents:

```typescript
// Replace UUID values with actual IDs after running Quick Start inserts:
//   SELECT id, content FROM document_chunks ORDER BY created_at;
const GOLDEN_SET: Array<{ query: string; expectedChunkIds: string[] }> = [
  {
    query: "how does approximate nearest neighbor search work?",
    expectedChunkIds: ["<replace-with-chunk-id-from-Quick-Start>"],
  },
  {
    query: "what is pgvector used for in PostgreSQL?",
    expectedChunkIds: ["<replace-with-chunk-id-from-Quick-Start>"],
  },
  {
    query: "how does hybrid search combine vector and keyword search?",
    expectedChunkIds: ["<replace-with-chunk-id-from-Quick-Start>"],
  },
];
```

**recallAtK function:**

```typescript
// recall@k: fraction of expected chunks found in top-k retrieved results
function recallAtK(retrieved: string[], expected: string[], k: number): number {
  const topK = retrieved.slice(0, k);
  const hits = expected.filter((id) => topK.includes(id)).length;
  return hits / expected.length;
}
```

**reciprocalRank function:**

```typescript
// MRR: reciprocal rank of the first relevant result (1/rank, or 0 if not found)
function reciprocalRank(retrieved: string[], expected: string[]): number {
  for (let i = 0; i < retrieved.length; i++) {
    if (expected.includes(retrieved[i])) return 1 / (i + 1);
  }
  return 0;
}
```

**retrieve function** (queries the database, returns IDs ordered by similarity):

```typescript
async function retrieveIds(
  openai: OpenAI,
  db: PrismaClient,
  query: string,
  topK = 5,
): Promise<string[]> {
  const response = await openai.embeddings.create({
    model: "text-embedding-3-small",
    input: query,
  });
  const queryVector = pgvector.toSql(response.data[0].embedding);

  const results = await db.$queryRaw<{ id: string }[]>`
    SELECT id
    FROM document_chunks
    WHERE embedding_model_version = 'text-embedding-3-small-v1'
    ORDER BY embedding <=> ${queryVector}
    LIMIT ${topK}
  `;

  return results.map((r) => r.id);
}
```

**main() function:**

```typescript
async function main(): Promise<void> {
  const openai = new OpenAI();
  const db = new PrismaClient();

  console.log("RAG Evaluation\n" + "=".repeat(40));

  let totalRecall = 0;
  let totalMRR = 0;

  for (const { query, expectedChunkIds } of GOLDEN_SET) {
    const retrieved = await retrieveIds(openai, db, query);
    const recall = recallAtK(retrieved, expectedChunkIds, 5);
    const mrr = reciprocalRank(retrieved, expectedChunkIds);

    totalRecall += recall;
    totalMRR += mrr;

    console.log(`\nQuery: "${query}"`);
    console.log(`  recall@5: ${recall.toFixed(2)}  MRR: ${mrr.toFixed(2)}`);
    console.log(`  retrieved: ${retrieved.slice(0, 5).join(", ")}`);
  }

  const avgRecall = totalRecall / GOLDEN_SET.length;
  const avgMRR = totalMRR / GOLDEN_SET.length;

  console.log("\n" + "=".repeat(40));
  console.log(
    `Average recall@5: ${avgRecall.toFixed(2)}  (threshold: >0.80 for production)`,
  );
  console.log(`Average MRR:      ${avgMRR.toFixed(2)}  (threshold: >0.70)`);

  if (avgRecall < 0.6) {
    console.log(
      "\n⚠ recall@5 < 0.60 — investigate chunking strategy or embedding model",
    );
  } else if (avgRecall >= 0.8) {
    console.log("\n✓ recall@5 ≥ 0.80 — production-ready");
  } else {
    console.log(
      "\n~ recall@5 0.60–0.80 — acceptable; consider tuning chunk size or overlap",
    );
  }

  await db.$disconnect();
}

main().catch(console.error);
```

Total file: all sections above assembled into one complete TypeScript file. Approximately 80–100 lines.
</action>
<verify>
<automated>wc -l .agent/skills/rag-vector-search/eval.ts && grep -c "recallAtK\|reciprocalRank\|GOLDEN_SET\|retrieveIds\|main" .agent/skills/rag-vector-search/eval.ts</automated>
<manual>Confirm: file imports pgvector/prisma, GOLDEN_SET has 3 queries, recallAtK and reciprocalRank functions present, main() runs the evaluation loop and prints averages with thresholds</manual>
</verify>
<done>eval.ts exists with ≥ 60 lines. Contains GOLDEN_SET (3 queries), recallAtK(), reciprocalRank(), retrieveIds() using pgvector.toSql(), main() loop printing recall@5 and MRR with production thresholds (>0.80, >0.70).</done>
</task>

<task type="auto">
  <name>Task 2: Update references/ — HNSW schema in embedding-pipelines.md, RRF CTE in retrieval-patterns.md</name>
  <files>
    .agent/skills/rag-vector-search/references/embedding-pipelines.md
    .agent/skills/rag-vector-search/references/retrieval-patterns.md
  </files>
  <action>
Update both reference files. Extend and correct — do NOT replace working content.

**embedding-pipelines.md updates:**

1. **Schema section** — replace the existing `CREATE TABLE embeddings (...)` SQL block with the updated production schema that matches SKILL.md's Quick Start:
   - Table name changes from `embeddings` to `document_chunks`
   - Column `text` → `content`
   - Add `search_vector tsvector GENERATED ALWAYS AS (to_tsvector('english', content)) STORED` column
   - Replace `CREATE INDEX ON embeddings USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);` with HNSW index: `CREATE INDEX ON document_chunks USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);`
   - Add the GIN index for tsvector: `CREATE INDEX ON document_chunks USING gin(search_vector);`
   - Keep the model_version index, update to `document_chunks (embedding_model_version)`
   - Add a note after the HNSW index: `-- IVFFlat alternative for >1M rows (requires training): CREATE INDEX ON document_chunks USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);`

2. **chunkText function** — the existing function is correct (word-based with overlap). Add a note above it: `// Note: chunkSize is in words. For token-precise control, use tiktoken (Python) or approximate: 1 word ≈ 1.3 tokens.`

3. **Batch Embedding section** — keep as-is (it's correct). Update the insert SQL inside `embedBatch` (if there is raw SQL) to use `document_chunks` and `pgvector.toSql()` instead of `JSON.stringify`. If there is no raw SQL insert in this file, add a note: `// For upsert pattern with pgvector.toSql(), see SKILL.md Quick Start.`

**retrieval-patterns.md updates:**

1. **Hybrid Search section** — keep the existing weighted-score SQL, but:
   - Add a banner above it: `> **Prefer RRF (below) for production.** The weighted approach (`0.7 × vector + 0.3 × ts_rank`) conflates incompatible score scales. RRF is more principled and requires no normalization.`
   - Change the section heading from `## Hybrid Search (Vector + Keyword)` to `## Hybrid Search (Vector + Keyword)` and add a sub-heading `### Option A: Weighted Score (simple, not recommended for production)`
   - After the existing weighted SQL block, add a new sub-heading `### Option B: RRF Fusion (recommended)` with the full RRF CTE:

```sql
-- RRF (Reciprocal Rank Fusion) — rank-based, no score normalization needed
-- Source: Cormack et al. (2009); k=60 generalizes well across datasets
WITH vector_search AS (
  SELECT id, ROW_NUMBER() OVER (ORDER BY embedding <=> $1::vector) AS rank
  FROM document_chunks
  WHERE embedding_model_version = 'text-embedding-3-small-v1'
  LIMIT 20
),
text_search AS (
  SELECT id, ROW_NUMBER() OVER (ORDER BY ts_rank(search_vector, plainto_tsquery('english', $2)) DESC) AS rank
  FROM document_chunks
  WHERE search_vector @@ plainto_tsquery('english', $2)
  LIMIT 20
)
SELECT
  COALESCE(v.id, t.id) AS id,
  1.0 / (60 + COALESCE(v.rank, 1000)) + 1.0 / (60 + COALESCE(t.rank, 1000)) AS rrf_score
FROM vector_search v
FULL OUTER JOIN text_search t ON v.id = t.id
ORDER BY rrf_score DESC
LIMIT 10;
```

2. **Table name alignment** — update any SQL in retrieval-patterns.md that still references the old `embeddings` table to use `document_chunks`. Update column `text` to `content` where it appears in SQL.

3. **Similarity Threshold Calibration** — update table name in the existing SQL from `embeddings` to `document_chunks`. Keep the pattern otherwise intact.

4. **Re-ranking section** — keep as-is (it is still valid and useful).
   </action>
   <verify>
   <automated>grep "USING hnsw" .agent/skills/rag-vector-search/references/embedding-pipelines.md && grep "rrf_score" .agent/skills/rag-vector-search/references/retrieval-patterns.md</automated>
   <manual>Confirm: embedding-pipelines.md table name is document_chunks, HNSW index present with IVFFlat as alternative comment. retrieval-patterns.md has RRF CTE under "Option B" heading, old weighted SQL kept under "Option A" with deprecation note.</manual>
   </verify>
   <done>embedding-pipelines.md schema uses document_chunks table with HNSW index (IVFFlat as commented alternative). retrieval-patterns.md has RRF CTE as Option B (recommended), weighted addition as Option A with "not recommended for production" note. Both files use document_chunks consistently.</done>
   </task>

</tasks>

<verification>
After both tasks complete:

1. `wc -l .agent/skills/rag-vector-search/eval.ts` — must be ≥ 60 lines
2. `grep "GOLDEN_SET" .agent/skills/rag-vector-search/eval.ts` — must match
3. `grep "recallAtK\|reciprocalRank" .agent/skills/rag-vector-search/eval.ts` — must match both
4. `grep "recall@5 > 0.8\|recall@5 >= 0.8\|>0.80\|>= 0.80" .agent/skills/rag-vector-search/eval.ts` — must match (threshold)
5. `grep "USING hnsw" .agent/skills/rag-vector-search/references/embedding-pipelines.md` — must match
6. `grep "document_chunks" .agent/skills/rag-vector-search/references/embedding-pipelines.md` — must match
7. `grep "rrf_score" .agent/skills/rag-vector-search/references/retrieval-patterns.md` — must match
8. `grep "not recommended\|Prefer RRF" .agent/skills/rag-vector-search/references/retrieval-patterns.md` — must match
   </verification>

<success_criteria>

- eval.ts exists, ≥ 60 lines, runnable with `npx tsx`, golden set of 3 queries, recallAtK() and reciprocalRank() implemented, production thresholds printed (recall@5 > 0.80, MRR > 0.70)
- embedding-pipelines.md schema updated: table name is `document_chunks`, HNSW index is primary (IVFFlat as commented alternative), embedding_model_version column present
- retrieval-patterns.md: RRF CTE added as recommended Option B, weighted addition retained as Option A with deprecation note, table references updated to `document_chunks`
- Requirements RAG-03 (RRF in references/) and RAG-05 (eval.ts with recall@k + MRR) addressed
  </success_criteria>

<output>
After completion, create `.planning/phases/12-rag-vector-search/12-02-SUMMARY.md`
</output>
