---
phase: 11-foundation-skills
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - .agent/skills/prompt-engineering/SKILL.md
  - .agent/skills/prompt-engineering/references/anti-patterns.md
  - .agent/skills/prompt-engineering/references/evaluation-workflows.md
  - .agent/skills/prompt-engineering/references/golden-set/chain-of-thought.md
  - .agent/skills/prompt-engineering/references/golden-set/few-shot.md
  - .agent/skills/prompt-engineering/references/golden-set/output-format.md
  - .agent/skills/prompt-engineering/references/golden-set/eval.ts
autonomous: true
requirements:
  - PROMPT-01
  - PROMPT-02
  - PROMPT-03
  - PROMPT-04
  - PROMPT-05

must_haves:
  truths:
    - "Developer can copy the quick-start block and make a structured Claude API call using role/context/task/output anatomy within 2 minutes"
    - "Every pattern entry carries applies-to and last-verified-against frontmatter with valid model IDs (claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5-20251001)"
    - "Developer can apply chain-of-thought, few-shot, and output format specification patterns using real SDK calls with real model IDs (not pseudocode)"
    - "The anti-pattern catalogue has exactly 5 entries each in Before/After format with a corrected version"
    - "Developer can run npx ts-node eval.ts and see pass/fail results for 3 golden-set test cases without an external platform"
  artifacts:
    - path: ".agent/skills/prompt-engineering/SKILL.md"
      provides: "Quick-start + numbered sections (1. Setup, 2. Config, 3. Patterns, 4. Gotchas / Pitfalls) with applies-to schema documentation"
      min_lines: 80
    - path: ".agent/skills/prompt-engineering/references/anti-patterns.md"
      provides: "5 anti-patterns with Before/After code blocks"
      min_lines: 80
    - path: ".agent/skills/prompt-engineering/references/evaluation-workflows.md"
      provides: "Golden set architecture, eval.ts runner documentation, prompt versioning explanation"
      min_lines: 50
    - path: ".agent/skills/prompt-engineering/references/golden-set/eval.ts"
      provides: "Runnable TypeScript eval runner"
      min_lines: 40
    - path: ".agent/skills/prompt-engineering/references/golden-set/chain-of-thought.md"
      provides: "Golden set test case"
    - path: ".agent/skills/prompt-engineering/references/golden-set/few-shot.md"
      provides: "Golden set test case"
    - path: ".agent/skills/prompt-engineering/references/golden-set/output-format.md"
      provides: "Golden set test case"
  key_links:
    - from: ".agent/skills/prompt-engineering/SKILL.md"
      to: ".agent/skills/prompt-engineering/references/anti-patterns.md"
      via: "reference link in header callout"
      pattern: "anti-patterns\\.md"
    - from: ".agent/skills/prompt-engineering/SKILL.md"
      to: ".agent/skills/prompt-engineering/references/evaluation-workflows.md"
      via: "reference link in header callout"
      pattern: "evaluation-workflows\\.md"
    - from: ".agent/skills/prompt-engineering/references/evaluation-workflows.md"
      to: ".agent/skills/prompt-engineering/references/golden-set/eval.ts"
      via: "npx ts-node eval.ts invocation documented"
      pattern: "eval\\.ts"
---

<objective>
Rewrite the prompt-engineering skill to v1.2 depth standard: quick-start first, numbered sections, model-specific applicability frontmatter, Before/After anti-patterns, and a runnable golden-set evaluation architecture.

Purpose: The existing skill has the right structure but was authored to a thinner standard. It lacks a quick-start, the `applies-to:` frontmatter schema, the Before/After anti-pattern format, and the golden-set eval tooling required by PROMPT-02, PROMPT-04, and PROMPT-05.

Output: A complete, shippable prompt-engineering skill that developers can follow from quick-start through evaluation without any external platform.
</objective>

<execution_context>
@/home/ollie/.claude/get-shit-done/workflows/execute-plan.md
@/home/ollie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.agent/skills/prompt-engineering/SKILL.md
@.agent/skills/prompt-engineering/references/anti-patterns.md
@.agent/skills/prompt-engineering/references/evaluation-workflows.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite SKILL.md with quick-start, numbered sections, and applies-to schema</name>
  <files>.agent/skills/prompt-engineering/SKILL.md</files>
  <action>
Rewrite `.agent/skills/prompt-engineering/SKILL.md` in-place. Preserve the YAML frontmatter block (update description to reflect v1.2 content). The rewrite must stay under 500 lines.

**Structure (exact order):**

```
---
name: prompt-engineering
description: [updated to reflect v1.2 content]
---

# Prompt Engineering

> See references/anti-patterns.md for top-5 anti-patterns with Before/After examples. See references/evaluation-workflows.md for golden-set scoring and eval.ts runner.

## Quick Start

[Minimal Claude API call using @anthropic-ai/sdk — copy-paste ready, zero setup beyond API key. Use client.messages.create with system + messages, model: 'claude-sonnet-4-6', max_tokens: 1024. Show a concrete example: extract capitals from a sentence. Include: import Anthropic, const client = new Anthropic(), const response = await client.messages.create({...}), console.log result.]

## 1. Setup

### Installation
[npm install @anthropic-ai/sdk; ANTHROPIC_API_KEY env var]

### Model Selection
[Table: claude-opus-4-6 (most capable, complex reasoning, CoT), claude-sonnet-4-6 (balanced, recommended default), claude-haiku-4-5-20251001 (fast/cheap, classification, eval judge). Note: instruction-tuned models respond well to XML section delimiters <context>, <task> — Claude-specific behavior, GPT-4o uses plain text sections.]

## 2. Configuration

### Prompt Anatomy (role/context/task/output)
[The buildPrompt() function from existing SKILL.md — keep it, this is the role/context/task/output pattern for PROMPT-01. Show the TypeScript interface and the function returning {system, user}. Use full TypeScript types, not `any`.]

### applies-to Frontmatter Schema (PROMPT-02)
[Define the schema for tagging prompt patterns. Show a YAML block:
  applies-to: [claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5-20251001]
  last-verified-against: claude-sonnet-4-6
  verified-date: 2026-02-24
Explain what each field means. List valid values for applies-to. Explain: instruction-tuned models (all Claude) respond well to XML tags; reasoning models do not need explicit step-by-step instructions as strongly as smaller models.]

## 3. Patterns

[Small intro: each pattern carries applies-to frontmatter — check it before choosing a model.]

### Chain-of-Thought (CoT)
[applies-to block: [claude-opus-4-6, claude-sonnet-4-6], last-verified-against: claude-sonnet-4-6]
[Full TypeScript SDK call: client.messages.create, model: 'claude-sonnet-4-6', max_tokens: 2048 (CoT needs space), system: 'Think through this step by step before giving your final answer.', user: math problem. Note: claude-opus-4-6 extended thinking mode available for complex reasoning — use model: 'claude-opus-4-6' and add thinking parameter.]

### Few-Shot
[applies-to block: [claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5-20251001], last-verified-against: claude-sonnet-4-6]
[Full TypeScript SDK call using the existing messages array pattern with 2 example pairs then the real user message. Note: do NOT wrap userInput in quotes — injection vector. Keep from existing skill.]

### Output Format Specification (Structured Output)
[applies-to block: [claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5-20251001], last-verified-against: claude-sonnet-4-6]
[Full TypeScript SDK call using client.messages.parse + zodOutputFormat(SentimentSchema) from @anthropic-ai/sdk/helpers/zod. Show SentimentSchema = z.object({sentiment: z.enum([...]), confidence: z.number()}) and the messages.parse call. Note: use client.messages.parse (not messages.create) for structured output — handles parsing automatically. Alternative: jsonSchemaOutputFormat from @anthropic-ai/sdk/helpers/json-schema if Zod not in project.]

### Decision Matrix
[Keep existing decision matrix table from current SKILL.md — it is correct and useful.]

## 4. Gotchas / Pitfalls

[Numbered list format matching the requirement for a dedicated section:]

**1. Anti-Patterns** — See `references/anti-patterns.md` for top-5 output-degrading patterns with Before/After examples.

**2. Prompt Versioning Without Structure** — Each prompt variant should be a file. Git history is version history. Tag each file with `last-verified-against:` frontmatter to lock it to a model snapshot. Warning sign: single `prompts.ts` with inline string edits.

**3. Missing applies-to Tags** — A pattern that works on claude-opus-4-6 may not work as well on claude-haiku-4-5-20251001. Always tag patterns. Warning sign: pattern documented without `applies-to:` frontmatter.

**4. Temperature Sensitivity** — Deterministic tasks (extraction, classification) must use temperature 0. Temperature 0 reduces but does not eliminate variance — test with N=20 samples for critical tasks.
  </action>
  <verify>
    <automated>wc -l .agent/skills/prompt-engineering/SKILL.md && grep -c "applies-to" .agent/skills/prompt-engineering/SKILL.md && grep -c "Quick Start" .agent/skills/prompt-engineering/SKILL.md && grep "messages.parse\|messages.create" .agent/skills/prompt-engineering/SKILL.md | wc -l</automated>
    <manual>Confirm: SKILL.md has a Quick Start section, numbered sections 1-4, at least 3 applies-to blocks (one per pattern), real SDK calls (not pseudocode), and line count is under 500.</manual>
  </verify>
  <done>SKILL.md has Quick Start section, sections numbered 1-4, applies-to frontmatter schema documented in section 2, chain-of-thought/few-shot/output format patterns each have an applies-to block and full SDK call, Gotchas section references anti-patterns.md, line count under 500.</done>
</task>

<task type="auto">
  <name>Task 2: Upgrade anti-patterns.md to Before/After format, expand to 5 entries; create golden-set files and eval.ts</name>
  <files>
    .agent/skills/prompt-engineering/references/anti-patterns.md
    .agent/skills/prompt-engineering/references/evaluation-workflows.md
    .agent/skills/prompt-engineering/references/golden-set/chain-of-thought.md
    .agent/skills/prompt-engineering/references/golden-set/few-shot.md
    .agent/skills/prompt-engineering/references/golden-set/output-format.md
    .agent/skills/prompt-engineering/references/golden-set/eval.ts
  </files>
  <action>
**Part A — Rewrite anti-patterns.md (5 entries, Before/After format):**

Rewrite `.agent/skills/prompt-engineering/references/anti-patterns.md` with exactly 5 anti-patterns. Each entry: heading, 1-sentence description, BAD code block (TypeScript, labeled `// BEFORE — anti-pattern`), GOOD code block (labeled `// AFTER — corrected`), and a 1-sentence "Why it matters" note.

Anti-patterns to include (in order):
1. **Prompt Injection via User Input** — Concatenating user input into system prompt. BEFORE: `system: "You are a helpful assistant. User name: " + userName + ". " + userMessage`. AFTER: user data goes in the user turn only; system is static developer-controlled context.
2. **Instruction Drift in Long Conversations** — Model gradually ignores earlier constraints as context grows. BEFORE: send full message history with original constraints only at turn 1. AFTER: re-inject key constraints every N turns (show wrapper function that prepends constraints to messages array).
3. **Hallucination on Structured Output** — Using JSON.parse directly without schema validation. BEFORE: `const data = JSON.parse(response.content[0].text)`. AFTER: `client.messages.parse` with `zodOutputFormat(MySchema)` from `@anthropic-ai/sdk/helpers/zod`.
4. **Constraint Overload (Over-Specifying Output)** — Piling on constraints until the model optimizes for following them rather than quality. BEFORE: system prompt with 15+ bullet constraints on format, tone, length, style, persona. AFTER: keep system prompt to 5 or fewer constraints; move detailed format requirements to output schema; test that removing a constraint doesn't improve output before adding it.
5. **Lost in the Middle** — Critical instructions buried in the middle of a long prompt. BEFORE: long system prompt with the most important instruction in paragraph 4 of 7. AFTER: put critical instructions at the start or end of the system prompt; use XML tags (`<critical>`) to make them structurally distinct.

**Part B — Rewrite evaluation-workflows.md:**

Rewrite to cover: (1) golden set architecture explanation, (2) prompt versioning with git, (3) eval.ts runner usage. Structure:

```

# Evaluation Workflows

## Golden Set Architecture

[Explain: a folder of .md test case files + eval.ts runner. Developer runs: npx ts-node eval.ts from the golden-set/ directory. No external platform needed.]

## Test Case Format

[Show the .md frontmatter schema:
pattern: chain-of-thought
model: claude-sonnet-4-6
Then ## Input Prompt section (System: + User:) and ## Expected Output Criteria section (bullet list).]

## Running the Eval

[Show: npm install -D ts-node @types/node (one-time). Then: npx ts-node eval.ts. Output format: one line per test: "✓ chain-of-thought: PASS" or "✗ few-shot: FAIL — [reason]". Exit code 1 on any failure.]

## Prompt Versioning

[Explain: each prompt variant is a file in prompts/. Git history is version history. Every file has last-verified-against: frontmatter. When you change a prompt, create a new file or commit the change — don't edit in-place without committing. Show example file structure: prompts/summarize-v1.md, prompts/summarize-v2.md.]

## Scoring and Iteration

[Keep the existing scoring rubric content from evaluation-workflows.md — it is correct. Append after the new sections above.]

```

**Part C — Create golden-set/ directory and files:**

Create `.agent/skills/prompt-engineering/references/golden-set/chain-of-thought.md`:
```

---

pattern: chain-of-thought
model: claude-sonnet-4-6
applies-to: [claude-opus-4-6, claude-sonnet-4-6]
last-verified-against: claude-sonnet-4-6
verified-date: 2026-02-24

---

## Input Prompt

System: You are a math tutor. Think through this step by step before giving your final answer. Show your reasoning.

User: A train travels 120 miles in 2 hours. A car travels the same distance in 1.5 hours. How much faster is the car, in mph?

## Expected Output Criteria

- Contains step-by-step reasoning before the final answer (not just the final number)
- Correctly calculates train speed: 120 ÷ 2 = 60 mph
- Correctly calculates car speed: 120 ÷ 1.5 = 80 mph
- Final answer states the car is 20 mph faster
- Does not skip directly to the answer without showing intermediate steps

```

Create `.agent/skills/prompt-engineering/references/golden-set/few-shot.md`:
```

---

pattern: few-shot
model: claude-sonnet-4-6
applies-to: [claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5-20251001]
last-verified-against: claude-sonnet-4-6
verified-date: 2026-02-24

---

## Input Prompt

User (example 1): Classify sentiment: "Product arrived broken and customer service was useless."
Assistant (example 1): {"sentiment": "negative", "confidence": 0.97}

User (example 2): Classify sentiment: "Fast shipping and exactly what I ordered."
Assistant (example 2): {"sentiment": "positive", "confidence": 0.95}

User (real): Classify the sentiment of the following text.

Text: The packaging was a bit dented but the item inside was perfect.

## Expected Output Criteria

- Returns valid JSON with "sentiment" and "confidence" fields
- Sentiment value is one of: "positive", "negative", "neutral"
- Confidence is a number between 0 and 1
- Correctly identifies the mixed/neutral sentiment (dented packaging vs perfect item)
- Does not wrap the JSON in markdown code fences

```

Create `.agent/skills/prompt-engineering/references/golden-set/output-format.md`:
```

---

pattern: output-format-specification
model: claude-sonnet-4-6
applies-to: [claude-opus-4-6, claude-sonnet-4-6, claude-haiku-4-5-20251001]
last-verified-against: claude-sonnet-4-6
verified-date: 2026-02-24

---

## Input Prompt

System: You are a data extractor. Return only valid JSON matching this schema exactly:
{"title": string, "author": string, "year": number, "genre": string}
Do not include any explanation or markdown formatting.

User: Extract the book details from this text:
"The Midnight Library by Matt Haig was published in 2020. It's a contemporary fiction novel about second chances."

## Expected Output Criteria

- Output is valid JSON (parseable by JSON.parse without error)
- Contains all four required fields: title, author, year, genre
- title is "The Midnight Library"
- author is "Matt Haig"
- year is the number 2020 (not the string "2020")
- genre references fiction, contemporary fiction, or similar — not empty
- No wrapping markdown code fences or explanation text outside the JSON

````

Create `.agent/skills/prompt-engineering/references/golden-set/eval.ts`:

```typescript
import Anthropic from '@anthropic-ai/sdk';
import * as fs from 'fs';
import * as path from 'path';

const client = new Anthropic(); // reads ANTHROPIC_API_KEY from env

interface TestCase {
  filePath: string;
  pattern: string;
  model: string;
  inputPrompt: string;
  expectedCriteria: string[];
}

function parseTestCase(filePath: string): TestCase {
  const content = fs.readFileSync(filePath, 'utf-8');
  const lines = content.split('\n');

  // Parse frontmatter
  let pattern = '';
  let model = 'claude-sonnet-4-6';
  let inFrontmatter = false;
  let frontmatterDone = false;
  let i = 0;

  if (lines[0] === '---') {
    inFrontmatter = true;
    i = 1;
  }
  for (; i < lines.length && !frontmatterDone; i++) {
    if (lines[i] === '---') { frontmatterDone = true; continue; }
    if (inFrontmatter) {
      const [key, ...rest] = lines[i].split(':');
      if (key.trim() === 'pattern') pattern = rest.join(':').trim();
      if (key.trim() === 'model') model = rest.join(':').trim();
    }
  }

  // Extract Input Prompt section
  const inputStart = content.indexOf('## Input Prompt\n');
  const criteriaStart = content.indexOf('## Expected Output Criteria\n');
  const inputPrompt = content.slice(inputStart + '## Input Prompt\n'.length, criteriaStart).trim();

  // Extract criteria bullets
  const criteriaSection = content.slice(criteriaStart + '## Expected Output Criteria\n'.length).trim();
  const expectedCriteria = criteriaSection
    .split('\n')
    .filter((l) => l.startsWith('- '))
    .map((l) => l.slice(2).trim());

  return { filePath, pattern, model, inputPrompt, expectedCriteria };
}

async function runTestCase(testCase: TestCase): Promise<{ passed: boolean; reason: string }> {
  // Build messages from Input Prompt section
  const lines = testCase.inputPrompt.split('\n');
  let system = '';
  let userContent = '';

  for (const line of lines) {
    if (line.startsWith('System: ')) system = line.slice('System: '.length);
    else if (line.startsWith('User: ') || line.startsWith('User (real): ')) {
      userContent = line.replace(/^User \(real\): /, '').replace(/^User: /, '');
    } else if (userContent) {
      userContent += '\n' + line;
    }
  }

  const messages: Anthropic.MessageParam[] = [{ role: 'user', content: userContent.trim() }];
  const createParams: Anthropic.MessageCreateParamsNonStreaming = {
    model: testCase.model,
    max_tokens: 1024,
    messages,
  };
  if (system) createParams.system = system;

  const response = await client.messages.create(createParams);
  const output = response.content[0].type === 'text' ? response.content[0].text : '';

  // LLM-as-judge using cheap model
  const judgeResponse = await client.messages.create({
    model: 'claude-haiku-4-5-20251001',
    max_tokens: 256,
    messages: [{
      role: 'user',
      content: `Output to evaluate:\n"${output}"\n\nCriteria (ALL must be satisfied):\n${testCase.expectedCriteria.map((c, i) => `${i + 1}. ${c}`).join('\n')}\n\nDoes the output satisfy ALL criteria? Reply with exactly PASS or FAIL on the first line, then one sentence explaining why.`,
    }],
  });

  const verdict = judgeResponse.content[0].type === 'text' ? judgeResponse.content[0].text.trim() : 'FAIL';
  const passed = verdict.toUpperCase().startsWith('PASS');
  const reason = verdict.split('\n').slice(1).join(' ').trim() || verdict;

  return { passed, reason };
}

async function main() {
  const goldenSetDir = path.dirname(__filename);
  const testFiles = fs.readdirSync(goldenSetDir)
    .filter((f) => f.endsWith('.md'))
    .map((f) => path.join(goldenSetDir, f));

  if (testFiles.length === 0) {
    console.error('No .md test case files found in golden-set/');
    process.exit(1);
  }

  let failures = 0;

  for (const filePath of testFiles) {
    const testCase = parseTestCase(filePath);
    process.stdout.write(`  Running ${testCase.pattern}... `);
    try {
      const result = await runTestCase(testCase);
      if (result.passed) {
        console.log(`✓ PASS`);
      } else {
        console.log(`✗ FAIL — ${result.reason}`);
        failures++;
      }
    } catch (err) {
      console.log(`✗ ERROR — ${(err as Error).message}`);
      failures++;
    }
  }

  console.log(`\n${testFiles.length - failures}/${testFiles.length} passed`);
  if (failures > 0) process.exit(1);
}

main();
````

  </action>
  <verify>
    <automated>wc -l .agent/skills/prompt-engineering/references/anti-patterns.md && grep -c "BEFORE\|AFTER" .agent/skills/prompt-engineering/references/anti-patterns.md && ls .agent/skills/prompt-engineering/references/golden-set/ && wc -l .agent/skills/prompt-engineering/references/golden-set/eval.ts</automated>
    <manual>Confirm: anti-patterns.md has 5 sections each with a BEFORE and AFTER code block. golden-set/ contains chain-of-thought.md, few-shot.md, output-format.md, eval.ts. eval.ts imports Anthropic and has a main() function.</manual>
  </verify>
  <done>anti-patterns.md has exactly 5 anti-patterns with Before/After TypeScript code blocks. evaluation-workflows.md explains golden set architecture and prompt versioning. golden-set/ directory exists with 3 .md test cases and eval.ts. Each golden-set .md has applies-to and last-verified-against frontmatter, an Input Prompt section, and Expected Output Criteria bullets.</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `wc -l .agent/skills/prompt-engineering/SKILL.md` — must be under 500 lines
2. `grep "Quick Start" .agent/skills/prompt-engineering/SKILL.md` — must exist
3. `grep "applies-to" .agent/skills/prompt-engineering/SKILL.md | wc -l` — must be 3 or more (one per pattern)
4. `grep -c "BEFORE\|AFTER" .agent/skills/prompt-engineering/references/anti-patterns.md` — must be 10+ (5 pairs)
5. `ls .agent/skills/prompt-engineering/references/golden-set/` — must list chain-of-thought.md, few-shot.md, output-format.md, eval.ts
6. `grep "messages.parse\|messages.create" .agent/skills/prompt-engineering/SKILL.md | wc -l` — must be 3+ (real SDK calls, not pseudocode)
</verification>

<success_criteria>
The prompt-engineering skill is shippable when:

- SKILL.md opens with a copy-paste quick-start block using real SDK call + real model ID
- SKILL.md has numbered sections 1-4 with Gotchas as section 4
- Every pattern (CoT, few-shot, output format) has applies-to and last-verified-against frontmatter
- anti-patterns.md has 5 entries in Before/After format with TypeScript code blocks
- evaluation-workflows.md explains the golden set architecture and prompt versioning approach
- golden-set/ has 3 .md test cases and a runnable eval.ts
- All files combined represent a developer following the skill without needing to read external docs
  </success_criteria>

<output>
After completion, create `.planning/phases/11-foundation-skills/11-01-SUMMARY.md` using the summary template.
</output>
