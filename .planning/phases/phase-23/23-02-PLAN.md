<plan phase="23" plan="2">
  <overview>
    <phase_name>LLM Telemetry CSV Ingestion</phase_name>
    <goal>Create ingestion pipeline for LLM telemetry CSV logs with UPSERT idempotency</goal>
  </overview>

  <dependencies>
    <complete>Phase 23 Plan 1: GitHub Actions Data Ingestion</complete>
  </dependencies>

  <tasks>
    <task type="auto" priority="1">
      <name>Create CSV telemetry parser</name>
      <files>packages/telemetry/src/csv_parser.py</files>
      <action>
        Implement CSV parsing for telemetry logs:
        - Read CSV from configurable path (env var TELEMETRY_CSV_PATH, default: .telemetry/usage.csv)
        - Handle standard CSV format with headers
        - Parse fields: timestamp, model, prompt_tokens, completion_tokens, 
          task_success, task_type, duration_ms, notes
        - Map to database fields:
          - prompt_tokens → input_tokens
          - completion_tokens → output_tokens
          - task_success → success
          - task_type → prompt_name
          - notes → parse for phase info if needed
        - Validate required columns (timestamp, model)
        - Handle missing/invalid data gracefully (skip row, log warning)
        - Return list of dicts for ingestion
      </action>
      <verify>Parser correctly reads existing .telemetry/usage.csv</verify>
      <done>csv_parser.py with parse_telemetry_csv() and configurable path</done>
    </task>

    <task type="auto" priority="1">
      <name>Create LLM telemetry ingestion script</name>
      <files>packages/telemetry/src/ingest_telemetry.py</files>
      <action>
        Implement telemetry ingestion:
        - Accept CSV path from CLI arg or TELEMETRY_CSV_PATH env var
        - Default path: .telemetry/usage.csv
        - Parse rows into LlmCall models with prompt_name and error_message fields
        - UPSERT to database using on_conflict_do_nothing or on_conflict_do_update
          based on unique constraint (timestamp, session_id, prompt_name)
        - Generate row hash if session_id not available in CSV
        - Track and log ingestion summary (rows read, inserted, skipped, errors)
        - Support --dry-run flag to preview without writing
      </action>
      <verify>Script ingests telemetry to database without duplicates on re-run</verify>
      <done>ingest_telemetry.py with UPSERT logic and idempotency</done>
    </task>

    <task type="auto" priority="1">
      <name>Add telemetry CLI commands</name>
      <files>packages/telemetry/package.json</files>
      <action>
        Add scripts:
        - telemetry:ingest:csv [path] - optional path arg, env var fallback
        - telemetry:stats - show aggregate stats (tokens, cost, success rate)
        Ensure Python environment is activated (poetry run or venv)
      </action>
      <verify>pnpm run telemetry:ingest:csv works with and without path arg</verify>
      <done>Telemetry CLI commands with configurable path support</done>
    </task>

    <task type="auto" priority="2">
      <name>Create telemetry statistics queries</name>
      <files>packages/telemetry/src/queries.py</files>
      <action>
        Implement common queries:
        - tokens_by_model(since, until) - aggregated by model
        - cost_by_phase(phase) - estimated cost breakdown
        - cost_by_prompt_name(prompt_name) - granular cost analysis
        - success_rate_by_phase(phase) - success percentage
        - daily_cost_trend(days) - time-series for dashboard
        - error_summary(since) - failed calls with error_message grouping
      </action>
      <verify>Queries return correct aggregated data</verify>
      <done>queries.py with analytics queries for dashboard</done>
    </task>

    <task type="auto" priority="2">
      <name>Add integration tests for ingestion</name>
      <files>packages/telemetry/tests/integration/test_ingestion.py</files>
      <action>
        Implement tests:
        - Test CSV parsing with sample data including edge cases
        - Test database UPSERT - insert new rows
        - Test database UPSERT - skip duplicates on re-run
        - Test query functions return expected aggregates
        - Test configurable CSV path via env var
      </action>
      <verify>Integration tests pass</verify>
      <done>test_ingestion.py with coverage for idempotency</done>
    </task>

    <task type="auto" priority="3">
      <name>Document telemetry ingestion workflow</name>
      <files>packages/telemetry/README.md</files>
      <action>
        Document:
        - CSV format expectations and field mapping
        - Environment variables (TELEMETRY_CSV_PATH, GITHUB_TOKEN)
        - Ingestion commands and CLI options
        - UPSERT/idempotency behavior
        - Query examples for analytics
        - Integration with dashboard (Phase 25)
        - Data retention policy (6 months pruning)
      </action>
      <verify>README is clear and complete</verify>
      <done>Telemetry package README</done>
    </task>

  </tasks>
</plan>
